{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Emotions to Gif ",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdrRp5VyI39f",
        "colab_type": "text"
      },
      "source": [
        "Based on:\n",
        "https://github.com/Amol2709/EMOTION-RECOGITION-USING-KERAS/tree/master/emotion_recognition\n",
        "https://medium.com/@ee18m003/emotion-recognition-using-keras-ad7881e2c3c6\n",
        "\n",
        "Available on Google Colab: https://drive.google.com/file/d/1sr1AwsR5lTmn2chrWGobFJI4wOnzQqq6/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hidq3A5nI39h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import sys\n",
        "import io, requests, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IDxwVW6slAC",
        "colab_type": "text"
      },
      "source": [
        "Please download the github and put that into your google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2oFwyS8JEoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA2JYkwiKbWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/#Location of folder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch2lSYWUuuaL",
        "colab_type": "text"
      },
      "source": [
        "Load up models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXRs7DTHI39s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the face detector cascade, emotion detection CNN, then define\n",
        "# the list of emotion labels\n",
        "detector = cv2.CascadeClassifier('/content/drive/My Drive/Colab_Notebooks/NASA_Space_Apps_Challenge/gagarin-firmware-master/models/haarcascade_frontalface_default.xml')\n",
        "model = load_model('/content/drive/My Drive/Colab_Notebooks/NASA_Space_Apps_Challenge/gagarin-firmware-master/models/epoch_60.hdf5')\n",
        "EMOTIONS = [\"angry\", \"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUHluvWRI39y",
        "colab_type": "text"
      },
      "source": [
        "# Image analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58iWRILtI39z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6xS2RwWI398",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frame= cv2.imread('/content/drive/My Drive/Colab_Notebooks/NASA_Space_Apps_Challenge/gagarin-firmware-master/data_samples/sample_img.jpg')\n",
        "# resize the frame and convert it to grayscale\n",
        "frame = imutils.resize(frame, width=300)\n",
        "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "# initialize the canvas for the visualization, then clone\n",
        "# the frame so we can draw on it\n",
        "canvas = np.zeros((220, 300, 3), dtype=\"uint8\")\n",
        "frameClone = frame.copy()\n",
        "# detect faces in the input frame, then clone the frame so that\n",
        "# we can draw on it\n",
        "rects = detector.detectMultiScale(gray, scaleFactor=1.1,minNeighbors=5, minSize=(30, 30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "# ensure at least one face was found before continuing\n",
        "for i in range(0,len(rects)):\n",
        "    # determine the largest face area\n",
        "    #rect = sorted(rects, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
        "    (fX, fY, fW, fH) = rects[i]\n",
        "    # extract the face ROI from the image, then pre-process\n",
        "    # it for the network\n",
        "    roi = gray[fY:fY + fH, fX:fX + fW]\n",
        "    roi = cv2.resize(roi, (48, 48))\n",
        "    roi = roi.astype(\"float\") / 255.0\n",
        "    roi = img_to_array(roi)\n",
        "    roi = np.expand_dims(roi, axis=0)\n",
        "    # make a prediction on the ROI, then lookup the class# label\n",
        "    preds = model.predict(roi)[0]\n",
        "    label = EMOTIONS[preds.argmax()]\n",
        "    # loop over the labels + probabilities and draw them\n",
        "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
        "        # construct the label text\n",
        "        text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "        # draw the label + probability bar on the canvas\n",
        "        w = int(prob * 300)\n",
        "        cv2.rectangle(canvas, (5, (i * 35) + 5),(w, (i * 35) + 35), (40, 50, 155), -1)\n",
        "        cv2.putText(canvas, text, (10, (i * 35) + 23),cv2.FONT_HERSHEY_SIMPLEX, 0.45,(55, 25, 5), 2)\n",
        "    cv2.putText(frameClone, label, (fX, fY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (40, 50, 155), 2)\n",
        "    cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(140, 50, 155), 2)\n",
        "    # show our classifications + probabilities\n",
        "#cv2_imshow('image', frameClone)\n",
        "#cv2_imshow('image', frameClone)\n",
        "\n",
        "cv2_imshow(frameClone)\n",
        "cv2_imshow(canvas)\n",
        "\n",
        "# cleanup the camera and close any open windows\n",
        "cv2.waitKey(0) # PRESS ANY KEY TO EXIT\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHoMNW4xI3-D",
        "colab_type": "text"
      },
      "source": [
        "# Video analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N79wUW5ZI3-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "camera = cv2.VideoCapture('/content/drive/My Drive/Colab_Notebooks/NASA_Space_Apps_Challenge/gagarin-firmware-master/data_samples/Running/Running in Space00000135.mp4')\n",
        "#writer = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30,(640,480))\n",
        "\n",
        "out = cv2.VideoWriter('/same_data/output.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, size)\n",
        "\n",
        "#Define the place to put all the images\n",
        "path = '/content/drive/My Drive/Colab_Notebooks/NASA_Space_Apps_Challenge/gagarin-firmware-master/data_samples/Running/Gif images'\n",
        "\n",
        "sec =0\n",
        "frameRate = 1\n",
        "count = 1\n",
        "\n",
        "while True:\n",
        "    (grabbed, frame) = camera.read()\n",
        "    if not grabbed: break # end of video\n",
        "        \n",
        "        \n",
        "    # resize the frame and convert it to grayscale\n",
        "    frame = imutils.resize(frame, width=300)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # canvas to draw on it\n",
        "    canvas = np.zeros((220, 300, 3), dtype=\"uint8\")\n",
        "    frameClone = frame.copy()\n",
        "\n",
        "    width = 220\n",
        "    height = 300\n",
        "    size = (width, height)\n",
        "\n",
        "    rects = detector.detectMultiScale(gray, scaleFactor=1.1,minNeighbors=5, minSize=(30, 30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    # ensure at least one face was found before continuing\n",
        "    if len(rects) > 0:\n",
        "        # determine the largest face area\n",
        "        rect = sorted(rects, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
        "        (fX, fY, fW, fH) = rect\n",
        "        # extract the face ROI from the image, then pre-process\n",
        "        # it for the network\n",
        "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
        "        roi = cv2.resize(roi, (48, 48))\n",
        "        roi = roi.astype(\"float\") / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        \n",
        "        # make a prediction on the ROI, then lookup the class# label\n",
        "        preds = model.predict(roi)[0]\n",
        "        label = EMOTIONS[preds.argmax()]\n",
        "        # loop over the labels + probabilities and draw them\n",
        "        for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
        "            # construct the label text\n",
        "            text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "            # draw the label + probability bar on the canvas\n",
        "            w = int(prob * 300)\n",
        "            cv2.rectangle(canvas, (5, (i * 35) + 5),(w, (i * 35) + 35), (0, 0, 255), -1)\n",
        "            cv2.putText(canvas, text, (10, (i * 35) + 23),cv2.FONT_HERSHEY_SIMPLEX, 0.45,(255, 255, 255), 2)\n",
        "        # draw the label on the frame\n",
        "        cv2.putText(frameClone, label, (fX, fY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "        cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
        "    # show our classifications + probabilities\n",
        "    #cv2_imshow(frameClone)\n",
        "\n",
        "    #writing images to folder\n",
        "    cv2.imwrite(os.path.join(path, \"running\"+str(count)+\".jpg\"), frameClone)\n",
        "\n",
        "    count = count+1\n",
        "\n",
        "    \n",
        "    #out.write(frameClone)\n",
        "    print(\"Processing Video\")\n",
        "    \n",
        "    #cv2.imshow(\"Probabilities\", canvas)\n",
        "\n",
        "    #out.write(frameClone)\n",
        "\n",
        "    \n",
        "    # if the ’q’ key is pressed, stop the loop\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "# cleanup the camera and close any open windows\n",
        "camera.release()\n",
        "out.release()\n",
        "#out.release()\n",
        "#cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}