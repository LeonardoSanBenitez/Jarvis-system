{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/benitez/anaconda3/envs/devCPU/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Math manipulation\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Vizualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from io import BytesIO\n",
    "\n",
    "# Deep Learning\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.visualize import random_colors, apply_mask, find_contours, patches, Polygon  # Export workarround\n",
    "\n",
    "# Utils\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags:\n",
    "    deleteWeights = False\n",
    "    downloadWeights = False\n",
    "flags = Flags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance segmentation definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm usign RCNN arquitecture pre trained on MSCOCO dataset\n",
    "\n",
    "\n",
    "results = dictionary for each image that we passed do the detect()\n",
    "\n",
    "results[0] = prediction of the first image (in our case, th only one)\n",
    "\n",
    "The keys of the dictionary of note are as follows:\n",
    "\n",
    "* ‘rois‘: The bound boxes or regions-of-interest (ROI) for detected objects, bottom left and top right (y1, x1, y2, x2), in pixels\n",
    "* ‘masks‘: The masks for the detected objects.\n",
    "* ‘class_ids‘: The class integers for the detected objects.\n",
    "* ‘scores‘: The probability or confidence for each predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightPath = 'models/mask_rcnn_coco.h5'\n",
    "if flags.downloadWeights==True: \n",
    "    !wget -O $weightPath https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _display_instances(image, boxes, masks, class_ids, class_names,\n",
    "                    scores=None, title=\"\",\n",
    "                    skip=False,\n",
    "                    figsize=(16, 16), ax=None,\n",
    "                    show_mask=True, show_bbox=True,\n",
    "                    colors=None, captions=None, verbose=True, save_img=False, image_path=\"\"):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    title: (optional) Figure title\n",
    "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
    "    figsize: (optional) the size of the image\n",
    "    colors: (optional) An array or colors to use with each object\n",
    "    captions: (optional) A list of strings to use as captions for each object\n",
    "    \"\"\"\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
    "\n",
    "    _, ax = plt.subplots(1, figsize=figsize)\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = colors or random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        if class_ids[i]==1: color=(1.0, 0.0, 0.0)\n",
    "        elif skip: continue#color = (0.0,0.0,0.0)\n",
    "        else: color = (0.1,0.1,0.6)\n",
    "        #color = colors[i]\n",
    "        # Bounding box\n",
    "        if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "            continue\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        if show_bbox:\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                            alpha=0.7, linestyle=\"dashed\",\n",
    "                            edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(p)\n",
    "        # Label\n",
    "        if not captions:\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i] if scores is not None else None\n",
    "            label = class_names[class_id]\n",
    "            caption = \"{} {:.3f}\".format(label, score) if score else label\n",
    "        else:\n",
    "            caption = captions[i]\n",
    "        ax.text(x1, y1 + 8, caption, color='w', size=11, backgroundcolor=\"none\")\n",
    "        # Mask\n",
    "        mask = masks[:, :, i]\n",
    "        if show_mask:\n",
    "            masked_image = apply_mask(masked_image, mask, color)\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "            ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    if save_img:\n",
    "        plt.savefig(image_path,bbox_inches='tight', pad_inches=-0.5,orientation= 'landscape')\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    buff = BytesIO()\n",
    "    plt.savefig(buff, format=\"png\", bbox_inches='tight', pad_inches=-0.1, orientation= 'landscape')\n",
    "    buff.seek(0)\n",
    "    \n",
    "    plt.close('all')\n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:723: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:725: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/mrcnn/model.py:775: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# define the custom configuration (the other variables have good default values)\n",
    "class config(Config):\n",
    "    NAME = \"barnInstanceSegmentation\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 80\n",
    "    DETECTION_MIN_CONFIDENCE = 0.55\n",
    "    DETECTION_NMS_THRESHOLD = 0.1\n",
    "    DETECTION_MAX_INSTANCES = 150\n",
    "    #I think that the input size can be modified\n",
    "    \n",
    "# define 81 classes that the coco model knowns about\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'cow', 'cow', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "# define the model\n",
    "rcnn = MaskRCNN(mode='inference', model_dir='./models/', config=config())\n",
    "# load coco model weights\n",
    "rcnn.load_weights(weightPath, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instanceSegmentation(frame, save_img=True, image_path='foo.jpg'):\n",
    "    results = rcnn.detect([frame], verbose=0)\n",
    "    # show photo with bounding boxes, masks, class labels and scores\n",
    "    img = _display_instances(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), \n",
    "                                results[0]['rois'], \n",
    "                                results[0]['masks'], \n",
    "                                results[0]['class_ids'],\n",
    "                                class_names, \n",
    "                                results[0]['scores'],\n",
    "                                #title='Frame instance segmentation',\n",
    "                                verbose=0,\n",
    "                                save_img=save_img,\n",
    "                                image_path=image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face detection:\n",
    "https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import argparse\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/benitez/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detector = MTCNN()\n",
    "model = load_model('models/epoch_60.hdf5')\n",
    "EMOTIONS = [\"angry\", \"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
    "\n",
    "def emotionAnalysis(frame, file_name):\n",
    "    result_list = detector.detect_faces(frame)\n",
    "    for result in result_list:\n",
    "        # Draw Rectangle\n",
    "        x1, y1, width, height = result['box']\n",
    "        x2, y2 = x1+width, y1+height\n",
    "        frame = cv2.rectangle(frame, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        for key, value in result['keypoints'].items():\n",
    "            frame = cv2.circle(frame, value, 2, (0,0,255), 2)\n",
    "            \n",
    "        # Detect emotion\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        roi = gray[y1:y2, x1:x2]\n",
    "        roi = cv2.resize(roi, (48, 48))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        preds = model.predict(roi)[0]\n",
    "        i = preds.argmax()\n",
    "        label = \"%.2f\"%preds[i] + \" \" + EMOTIONS[i]\n",
    "        frame = cv2.putText(frame, label, (x1, y1),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        #save\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imsave(file_name, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run to test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 19.91584852734923 %\n",
      "---> 39.83169705469846 %\n",
      "---> 59.747545582047685 %\n",
      "---> 79.66339410939692 %\n",
      "---> 99.57924263674614 %\n"
     ]
    }
   ],
   "source": [
    "videoPath = 'data_samples/Running/'\n",
    "videoName = 'Running_in_Space00000135'\n",
    "save_img = True # True=the function above will already save the image, otherwise we save ourself\n",
    "n=5 # number of frames to take from this video\n",
    "\n",
    "##################\n",
    "\n",
    "#os.system('mkdir temp_out')\n",
    "camera = cv2.VideoCapture(videoPath+videoName+'.mp4')\n",
    "i=0\n",
    "total = camera.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "interval = int(math.floor(camera.get(cv2.CAP_PROP_FRAME_COUNT)/n))\n",
    "while True:\n",
    "    (grabbed, frame) = camera.read()\n",
    "    i+=1\n",
    "    if not grabbed: break # end of video\n",
    "    if i%interval: continue #skip frame\n",
    "        \n",
    "    instanceSegmentation(frame, save_img=save_img, image_path='temp_out/'+str(i)+'.jpg')\n",
    "    #emotionAnalysis(frame, 'temp_out/'+str(i)+'.jpg')\n",
    "    \n",
    "    print('--->', 100*i/total, '%')\n",
    "#os.system('zip -r ' + videoName + ' temp_out')\n",
    "#os.system('rm -r temp_out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
